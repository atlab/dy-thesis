\subsection{Simulation}
We next demonstrate how the most efficient among different regularized estimators can reveal the structure of correlations.
We constructed four families of $50\times 50$ covariance matrices, each with structure that matched one of the four regularized estimators (Fig.~\ref{fig:1} Row 2, A--D and Methods).  We used these covariance matrices as the ground truth in multivariate normal distributions with zero means and drew samples of various sizes. Even with relatively large sample sizes (\emph{e.g.}\;$n=500$), the sample correlation matrices were contaminated by sampling noise and their underlying structures were difficult to discern (Fig.~\ref{fig:1} Row 3).

The evaluation of any covariance matrix estimator, $C$, is performed with respect to a \emph{loss function} $\loss{C,\Sigma}$ to quantify its discrepancy from truth, $\Sigma$.  The loss function must attain its minimum when $C=\Sigma$.

In this study, we adopted the \emph{normal loss} function defined as
\begin{equation}\label{eq:loss}
    \loss{C,\Sigma} = \frac 1 p\left[\ln \det C + \Tr(C^{-1}\Sigma)\right]
\end{equation}

This loss function is related to the multivariate normal log-likelihood function $L(\Sigma|C_{\sf sample})$,
\begin{equation}
    \loss{X,Y} \equiv -\ln(2\pi) - \frac 2 {pn} L(X|Y)
\end{equation}
Normalization by $p$ and $n$ makes the values of the loss function comparable across different population sizes and sample sizes.

Although the two functions are similar in form, they differ conceptually: The log-likelihood $L(\Sigma|C_{\sf sample})$ is a function of the parameter $\Sigma$ given the sample covariance matrix $C_{\sf sample}$ whereas the loss function $\loss{C,\Sigma}$ expresses the deviation of an arbitrary estimate $C$ from the truth $\Sigma$. Minimum expected loss may be found at a point that is far removed from the point of maximum likelihood given the same data.

The choice of the normal loss function is motivated, in part, by mathematical convenience. We expect that the main conclusions of our study will not change qualitatively with other well behaved loss functions, such as Stein's entropy loss or quadratic loss \citep{James:1961, Fan:2008, Ledoit:2004, Schafer:2005}.

We drew 30 independent samples of sizes $n=250$, 500, 1000, 2000, and 4000 from each model and computed the \emph{excess loss} $\loss{C,\Sigma}-\loss{\Sigma,\Sigma}$ for each of the five estimators.  The hyperparameters of the regularized estimators were optimized by nested cross-validation using only the data in the sample.  All the regularized estimators produced better estimates (lower excess loss) than the sample covariance matrix.  However, estimators whose structure matched the true model outperformed the other estimators (Fig.~\ref{fig:1} Row 5).  Note that when the ground truth had zero correlations (Column A), $C_{\sf factor}$ performed equally well to $C_{\sf diag}$ because it correctly identified zero factors and only estimated the individual variances. Similarly, when the number of latent units was zero (Column C), $C_{\sf sparse+latent}$ performed equally well to $C_{\sf sparse}$ because it correctly identified zero latent units.

With increasing sample sizes, all estimators converged to the truth (zero excess loss).  However, the discriminability between their performances increased with sample size (data not shown).

With empirical data, the ground truth, $\Sigma$, is not accessible and excess loss cannot be computed directly. Instead, the loss function may be estimated from the sample through \emph{validation}.  In a validation procedure, an independent \emph{testing sample} is set aside to compute an additional sample covariance estimate $C_{\sf sample}^\prime$ to validate the estimate $C$ computed from the  \emph{training sample}.   The estimate $C_{\sf sample}^\prime$ is used to compute the \emph{validation loss} $\loss{C,C_{\sf sample}^\prime}$, which approximates the loss  $\loss{C,\Sigma}$.

The normal loss (Eq.~\ref{eq:loss}) is particularly suitable because it is additive in its second argument, \emph{i.e.}
\begin{equation*}\label{eq:additivity}
    \loss{C,Z_1} + \loss{C,Z_2} \equiv \loss{C,Z_1+Z_2}
\end{equation*}
Then, with fixed $C$, validation loss is an unbiased estimate of true loss:
\begin{equation*}
    \E{\loss{C,C_{\sf sample}^\prime}}=\loss{C,\E{C_{\sf sample}^\prime}}=\loss{C,\Sigma}.
\end{equation*}

The property of additivity does not hold for other popular loss functions such as Stein's entropy loss or various quadratic losses; their validation losses are biased estimates of the loss function and validation does not exactly evaluate a discrepancy from truth.

The loss function above assumes a single covariance matrix across all conditions in the dataset.  However, neuroscientists often estimate a common correlation matrix across multiple stimulus conditions when the variances of responses are conditioned on the stimulus \citep{Vogels:1989, Ponce:2013}. In this case, the stimulus-dependent means and variances are estimated from the training dataset along with a common correlation matrix. An adaptation of the normal loss function to this case is described in Methods.

Acquiring a separate testing sample is not practical. Instead, $K$-fold cross-validation can be used: The sample is divided into $K$ subsets of approximately equal size ($K=10$ in this study).  Then each subset is used as the testing sample with the other $K-1$ subsets serving as the training sample. The validation losses from all such `folds' are averaged to produce the \emph{cross-validation loss}.

Cross-validation loss accurately reproduced the relationships between the excess losses of the estimators (Fig.~\ref{fig:1} Row 6), confirming that cross-validation can evaluate true loss.

This simulation study demonstrated that, with sufficiently large sample sizes, imposing the correct type of structure on the estimate leads to greater estimation improvement than imposing other structures.
